{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: Detecting Breast Cancer â€” Classifying Tumors ðŸ©º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name:\n",
    "\n",
    "Student ID:\n",
    "\n",
    "Collaborators:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "For this homework, work through **Lab 5 (Detecting Breast Cancer â€” Understanding Logistic Regression)** first. Many of the things we ask you to do in this homework are explained in the lab. In general, you should feel free to import any package that we have previously used in class. Ensure that all plots have the necessary components that a plot should have (e.g. axes labels, a title, a legend).\n",
    "\n",
    "Frequently **save** your notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborators and Sources\n",
    "Furthermore, in addition to recording your **collaborators** on this homework, please also remember to **cite/indicate all external sources** used when finishing this assignment. \n",
    "> This includes peers, TAs, and links to online sources. \n",
    "\n",
    "Note that these citations will be taken into account during the grading and regrading process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collaborators and sources:\n",
    "# Albert Einstein and Marie Curie\n",
    "# https://developers.google.com/edu/python/strings\n",
    "\n",
    "# your code here\n",
    "answer = 'my answer'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission instructions\n",
    "* Submit this python notebook including your answers in the code cells as homework submission.\n",
    "* **Do not change the number of cells!** Your submission notebook should have exactly one code cell per problem. \n",
    "* Do **not** remove the `# your code here` line and add you solution after that line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Predicting Breast Cancer\n",
    "Since all of this looks pretty good now, we can turn to our actual application for _this week_: predicting whether a patient has breast cancer or not. First, let's take a quick look at the data from the University of Wisconsin. Each data point contains information about the breast cancer cells of a single patient derived from a digitized image of a fine needle aspirate (FNA) of a breast mass, similar to those: \n",
    "\n",
    "![fine needle aspirate](utility/pics/fna.jpg)\n",
    "\n",
    "Note that we do not have the image data, but the features capture the cell shapes and we also have the diagnosis (either `malignant` or `benign`), which we will treat as the class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Dataset\n",
    "\n",
    "Let's take a look at what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer_data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.1\n",
    "\n",
    "Let's take a second to explore `data`.\n",
    "\n",
    "**Try this!** In the following cell, evaluate each of the fields in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write-up!** Just for safe-keeping, how many data points are there in this dataset? How many features? What are the names of the features? What are the names of the classes and how are they encoded in the labels?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.2\n",
    "\n",
    "Now that we are familiar with what the data looks like, let's pull out what we need from it.\n",
    "\n",
    "**Try this!** Pull out the input data `X` and label (classification) data `y` from `data`. Make sure that the labels in `y` are either `+1` for `malignant` or `-1` for `benign` tumors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "N, d = X.shape\n",
    "f'There are {N} data points with {d} features each.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.3\n",
    "\n",
    "Let's plot the distribution of the classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utility.util import configure_plots\n",
    "\n",
    "configure_plots()\n",
    "\n",
    "classes, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "print(counts)\n",
    "print(classes)\n",
    "\n",
    "plt.bar(['malignant ($+1$)', 'benign ($-1$)'], counts)\n",
    "plt.title('Class Distribution - Breast Cancer Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write-up!** What do you notice about the number of observations in each class? Could this potentially lead to problems for our modeling? Think about the data (specifically, the subsets) of data we use to train the data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train any model, we split our data into two or more subsets for training, validation, and testing. The [`train_test_split` ðŸ”—](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function provided by Scikit-Learn has a keyword argument called `stratify`.\n",
    "\n",
    "**Write-up!** What does the `stratify` argument do according to the function's documentation? Why do we use `stratify` when creating our training and testing sets for this dataset? How would our evaluation of the model be affected if we did not use stratify?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Logistic Regression Model\n",
    "\n",
    "Now, you will train the classifier and then evaluate its performance. \n",
    "\n",
    "In the lab, we implemented Logistic Regression as described and formulated in class. In this series of problems, we will first review the model and practice building one with `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.4\n",
    "\n",
    "First, we will begin by creating training and test sets with which we will build and evaluate our model with. Then, we will create a new instance of a `LogisticRegression` model and fit it with our training set. As we noticed in [Problem 1.3](#Problem-1.3), our dataset does not have a balanced class distribution. We will need to handle this accordingly.\n",
    "\n",
    "**Try this!** In the following cell, you should do the following:\n",
    "1. Creates a train/test split with `train_test_split` with a `test_size` of 0.3, stratification by `y` with `stratify`, and a `random_state` of 4.\n",
    "2. Creates a new `LogisticRegression` model with the `'liblinear'` `solver` and `fit` it with the training set and stores it in `linear_model`.\n",
    "3. Evaluate the performance by manually computing and printing the classification _accuracy_ of your trained model on the test set.\n",
    "\n",
    "> **Hint**: Feel free to refer to any official documentation and remember that you can easily view the documentation in your notebook for any function or object by adding a `?` after its name (eg. `train_test_split?`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Diving Deeper into Classification Error\n",
    "\n",
    "While our model's accuracy was pretty good, let's analyze this accuracy more closely. For classification problems we can, instead of just counting mistakes, look at which kind of mistakes we made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.1\n",
    "\n",
    "**Write-up!** What are the two different kinds of mistakes we can make for breast cancer prediction? _**Hint**: Think about the decision that our model is making._"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write-up!** Recall that in Lab3 we counted false positives and false negatives. What are false positives and false negatives and why do we care to distinguish between them when they are both errors made by our classifier model? In the context of classifying tumors as either malignant or benign, does it make sense to prefer one type of mistake over the other? If so, which one and why? If not, also explain why?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices\n",
    "\n",
    "A [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix), sometimes called an error matrix, helps us summarize and understand the performance of our classification models by presenting the different types of correct and incorrect predictions our model is making: true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "<img alt=\"confusion matrix\" src=\"utility/pics/confusion-matrix.png\" />\n",
    "\n",
    "â€” [Source](https://jkmsmkj.blogspot.com/2018/10/confusion-matrix.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.2\n",
    "\n",
    "**Try this!** In the following cell, use `confusion_matrix` from `sklearn.metrics` to compute the confusion matrix for the model you built in [Problem 1.4](#Problem-1.4). Then, use `plot_confusion_matrix` to visualize the matrix (you will need to specify the `labels` to use). `HINT` Again, feel free to refer to any official documentation and remember that you can easily view the documentation for a function or object by adding a `?` after its name (eg. `confusion_matrix?`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from utility.util import plot_confusion_matrix\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Making Decisions\n",
    "\n",
    "Now, let's take a look at decision boundaries. As we mentioned in both lecture and lab, label predictions (eg. $\\hat{y} = +1$) are made by considering the class probabilities $P(y = +1 \\mid x)$ produced by our model with a decision threshold. Typically, we use a threshold of 0.5 such that $\\hat{y} = +1$ if $P(y = +1 \\mid x) > 0.5$. However, as we hinted in [Problem 2.1](#Problem-2.1), there are situations and contexts in which the cost of making a _false positive_ error is greater than making a _false negative_ error. We can account for cost differences by adjusting the threshold, and therefore the decision boundary, of our classifier.\n",
    "\n",
    "In the following few cells, we will prepare an experiment to test and observe the effects of changing the decision threshold of our classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3\n",
    "\n",
    "The first thing we will need to prepare is a way to compute the predictions of a model given a threshold. This is because `sklearn` does not offer an option to do this.\n",
    "\n",
    "**Try this!** Complete the `make_predictions` function so that it computes the predicted labels `y_pred` $\\in [-1, 1]$ for data `X_test` with a given `model` and decision `threshold`. `HINT` You may find the `model.predict_proba` method and [this StackOverflow thread](https://stackoverflow.com/questions/30820962/splitting-columns-of-a-numpy-array-easily) to be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, X_test, threshold=0.5):\n",
    "    '''\n",
    "    Computes the predicted labels for points in X_TEST using a given MODEL and THRESHOLD\n",
    "    '''\n",
    "    \n",
    "    # your code here\n",
    "\n",
    "    \n",
    "    assert isinstance(y_pred, np.ndarray), \\\n",
    "        'Please return your predictions as an NumPy array'\n",
    "    assert np.all(np.isin(np.unique(y_pred), [-1, 1])), \\\n",
    "        'Your predictions should be either -1 or +1'\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.4\n",
    "\n",
    "Next, we will combine all of the things we have done in earlier problems into a single function that produces a confusion matrix plot by fitting and evaluating new Logisitic Regression models with varying thresholds. Note that there will be some redundancy/inefficiency in this function because it repeatedly recomputes the same values, but it is okay as it simplifies the code.\n",
    "\n",
    "**Try this!** Complete the `boundary_experiment` function so that it does the following:\n",
    "1. Creates a train/test split with `train_test_split` with a `test_size` of 0.3, stratification by `y` with `stratify`, and a `random_state` of 11.\n",
    "2. Creates a new `LogisticRegression` model with the `'liblinear'` `solver` and `fit` it with the training set.\n",
    "3. Makes predictions `y_pred` for test set points using `make_predictions` and `threshold`.\n",
    "4. Computes a confusion matrix and then plots it using `plot_confusion_matrix`.\n",
    "\n",
    "`HINT` Feel free to refer to any official documentation and remember that you can easily view the documentation for a function or object by adding a `?` after its name (eg. `plot_confusion_matrix?`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_experiment(X, y, threshold=0.5):\n",
    "    \n",
    "    # your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.5\n",
    "\n",
    "In the following cell, we will experiment with how changing the decision boundary (threshold) affects the confusion matrix using the functions that you implemented earlier. In order to make the visualization work you will need to install an additional package, `ipywidgets`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jupyter Lab\n",
    "If you are using Jupyter Lab, you will need to install [node.js](https://nodejs.org/en/) first. You can do this via [brew](https://brew.sh).\n",
    "\n",
    "```\n",
    "$ brew install node\n",
    "```\n",
    "\n",
    "Then, you execute\n",
    "\n",
    "```\n",
    "$ jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "```\n",
    "as described in this [this article](https://ipywidgets.readthedocs.io/en/stable/user_install.html#installing-the-jupyterlab-extension). Then, restart Jupyter Lab\n",
    "\n",
    "#### Jupyter Notebook or Anaconda\n",
    "If you are using Jupyter Notebook or Anaconda, then follow the instructions described in this [this article](https://ipywidgets.readthedocs.io/en/stable/user_install.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Evaluate the following cell and experiment with the slider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interactive, fixed\n",
    "\n",
    "interactive(boundary_experiment, X=fixed(X), y=fixed(y), threshold=(0, 1, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write-up!** How does changing the threshold affect the confusion matrix (eg. correct classifications and errors)? What is the smallest threshold, if any, that eliminates false positive errors? What is the largest threshold, if any, that eliminates false negative errors? In each case, explain whether or not we would want to do so?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. An Explainable Classifier: Decision Trees\n",
    "\n",
    "With the rapid development and advancement in artificial intelligence (AI) technologies, including the machine learning that we use in this class, there is an increasing concern over the ethics of AI. There are often situations where AI is used and in which it is critically important to be sure that a model is both reliable and correct. For many models, it is often difficult to explain why they produce the results that they do. These concerns have led to much debate and the advent of the [Explainable AI (XAI)](https://en.wikipedia.org/wiki/Explainable_Artificial_Intelligence) movement. The goal of XAI is to promote the development of AI whose actions and decisions can be easily understood by humans. Though not originally designed with XAI in mind, decision trees have increasingly become the focus of attention within this movement.\n",
    "\n",
    "Perhaps, this is an alternative model we can use to predict breast cancer.\n",
    "\n",
    "In the following section, we will introduce and explore, though not to great depths, what decision trees are and how they can be used as explainable classification models. We will later use this model to explore model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Do They Work?\n",
    "\n",
    "Intuitively, a decision tree classifier tries to determine the class of a data point by making a series of \"tests\". Consider the following diagram of a decision tree that attempts to predict whether it will rain:\n",
    "\n",
    "![dtree](utility/pics/decision-tree.png)\n",
    "\n",
    "â€” [Source](https://prateekvjoshi.com/2016/03/22/how-are-decision-trees-constructed-in-machine-learning/)\n",
    "\n",
    "At each level (or depth), a feature of the data is examined and and a decision is made. The outcomes of that decision are represented as branches of the tree. In the tree above, the first feature that is examined is temperature, and if the temperature is greater 70 degrees we will do one thing and if it is less than 70, we will do another thing.\n",
    "\n",
    "What makes decision trees explainable is that we, as humans, can read off and follow exactly how the decision tree came to it's conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Decision Trees\n",
    "\n",
    "We will not discuss how decision trees are built in this course. Instead, we will opt to do do that for you. The following cell contains a function that takes a training set and produces a fit decision tree classification model.\n",
    "\n",
    "The main thing to know about decision trees is that their complexity is directly determined by their depth. As we mentioned before, each level of depth in the tree is another set of decisions that the model can make. By increasing the depth of the tree, we increase the complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def make_decision_tree(X_train, y_train, depth=None):\n",
    "    return DecisionTreeClassifier(max_depth=depth, random_state=4).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's try to build a decision tree to classify breast cancer tumors. We will reuse the training and testing sets we made earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = make_decision_tree(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Decision Tree\n",
    "\n",
    "Again, the selling point of decision trees is that they are readily interpretable. Let's take a look at what our model looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.1\n",
    "\n",
    "The following cell produces text and encodes what the tree looks like. We will use a online tool called [Viz.js](http://viz-js.com/) to visualize the results.\n",
    "\n",
    "**Try this!** Run the following cell and copy/paste the outputs in to [Viz.js](http://viz-js.com/). Then clear the outputs of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "encoding = export_graphviz(model, impurity=False, filled=True, rounded=True,\n",
    "                           feature_names=data.feature_names,\n",
    "                           class_names=data.target_names)\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write-up!** What was the maximum depth of the decision tree?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.2\n",
    "\n",
    "Let's evaluate the performance of the \"max-depth\" decision tree we just visualized.\n",
    "\n",
    "**Try this!** In the cell below, compute the accuracy of the `tree_model` on our testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write-up!** How does this compare to the performance of our linear model?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Selection\n",
    "In the lecture we saw that higher-order polynomials can give us more complex models. Typically increasing model complexity can help improving the performance. However, only to a certain extent. At some point you start to overfit and the performance on a held-out test set will start to decrease again. For decision trees model complexity increases with the depth of the trees used. It is pretty easy to see that by using a sufficient number of splits, you can classify the entire training set correctly. That is classic _overfitting_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.1\n",
    "\n",
    "**Try this!** In the cell below, compute the training and validation error of decision tree classifiers with depths from `1` to `10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_rest, X_test, y_rest, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.3, stratify=y, random_state=10)\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "    train_test_split(X_rest, y_rest, test_size=0.15, stratify=y_rest, random_state=10)\n",
    "\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Plot the training and validation error curves against the depth that produced them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write-up!** Which model (which depths) would you choose and why? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.2\n",
    "Evaluate the model you selected above by computing the test error. Print the test error. \n",
    "> Hint: You will have to retrain your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSE217",
   "language": "python",
   "name": "cse217"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
