{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: K-Means Clustering ğŸ—‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "* Calculate distance between points using Euclidean and Manhattan method\n",
    "* Understand k-means clustering algorithm\n",
    "* Visualize clusters\n",
    "* Understand how to find appropriate k value from sum of squared errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today's Lab\n",
    "\n",
    "Today we will be taking our first look at clustering as a machine learning topic â€” specifically, [$k$-means](https://en.wikipedia.org/wiki/K-means_clustering). K-means is an [unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning) method, which means that it doesn't need to have \"true\" labels or examples to learn a pattern. Instead, this class of algorithms can be described as pattern recognizers in the truest sense.\n",
    "\n",
    "In this lab, we will first try our hand at implementing the algorithm, during which we will investigate how it works and explore some of the caveats to know when you use it. Then we will end by discussion how to choose the right number of clusters for your data.\n",
    "\n",
    "By the end of the lab, you should have a pretty decent understanding of how $k$-means works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. [Implementing K-means](#1.-Implementing-K-means)\n",
    "2. [The Initial Centroids Matter!](#2.-The-Initial-Centroids-Matter!)\n",
    "3. [Deciding How Many Clusters to Use](#3.-Deciding-How-Many-Clusters-to-Use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implementing K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Clustering?\n",
    "\n",
    "As usual, I'm just going to have Wikipedia do the honors of explaining what clustering is. Here is a quote from [Cluster Analysis](https://en.wikipedia.org/wiki/Cluster_analysis):\n",
    "\n",
    "> Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters).\n",
    "\n",
    "A common theme in our discussion of machine learning so far is the fact that we try to build models of the world that can describe specific phenomena. We want to be able to extract the patterns encoded by our observations (data) to study how a process works and potentially predict how that process might work in the future. In the case of $k$-means, we seek to systematically extract information about the distribution of our data by answering questions like \"do our observations naturally form groups?\"\n",
    "\n",
    "![clustering](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Cluster-2.svg/220px-Cluster-2.svg.png)\n",
    "\n",
    "Clustering, and especially $k$-means, is not just another algorithm, but can actually be a useful addition to any EDA process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Our Toy Data\n",
    "\n",
    "As usual, we will use some toy data to help us with our implementation. First, let's load the packages we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utility.util import configure_plots\n",
    "\n",
    "configure_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help facilitate this process we have provided some toy data for you to use. In the cell we import the `load_toy` function, into which we can pass the number of points we would like to have $N$ and the number of clusters we would like to have $k$, among other values. This function will then return data points $X$ and another value that we don't really need to worry about now.\n",
    "\n",
    "> **For the interested!** Feel free to check out how this data is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.util import load_toy\n",
    "\n",
    "n, k = 300, 3\n",
    "X, _ = load_toy(n, k, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what we get.\n",
    "\n",
    "**Try this!** In the cell below, produce a scatterplot of the data $X$. As a review, also be sure to add proper plot components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Our Centroids\n",
    "\n",
    "But wait what are centroids? In the $k$-means terminology, centroids are the centers of our clusters and will be what we use to determine to which cluster some point $x$ belongs.\n",
    "\n",
    "While there are many ways to initialize our centroids, a _not-too-bad_ way to go about it is to randomly sample them from from our data. Later on, we will take a look at what you'll need to be aware of when choosing these points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Complete the `sample_centroids` function so that it randomly samples and returns $k$ points from $X$ that we can use as our initial centroids. To make things easy, please return the centroids as a NumPy array with a structure similar to the $X$ matrix. _**Hint**: You can choose (sample) from an array using using [`np.random.choice` ğŸ”—](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.choice.html)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_centroids(X, k, random_state=None):\n",
    "    '''Sample and return K data points from X'''\n",
    "    \n",
    "    if random_state:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    # your code here\n",
    "\n",
    "    \n",
    "    assert isinstance(centroids, np.ndarray), 'Your centroids should be in a NumPy array'\n",
    "    assert centroids.shape == (k, X.shape[1]), f'Your centroids should have shape ({k}, {X.shape[1]})'\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = sample_centroids(X, k, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at which points where chosen.\n",
    "\n",
    "**Try this!** In the cell below, recreate the scatter plot of $X$ you made earlier. Then scatter the centroids on top of it. Make sure that the centroids are obviously visible. `Hint` consider changing the size, marker, and color of the centroid points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Measures\n",
    "\n",
    "Now that we have centroids, the next thing will want to do is to be able to compute the similarity (or distances) between a group of points to a centroid. In this section, we will implement [Euclidean](https://en.wikipedia.org/wiki/Euclidean_distance) and [Manhattan](https://en.wikipedia.org/wiki/Taxicab_geometry) distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean Distance\n",
    "\n",
    "I'm sure you are all familiar with this distance measure from life. Here is the formula: $$d(\\pmb{p}, \\pmb{q}) = \\| \\pmb{p} - \\pmb{q} \\| = \\sqrt{\\sum_{j=1}^{d} (p_j - q_j)^2}$$\n",
    "\n",
    "**Try this!** Complete the `euclidean` function so that it computes the [Euclidean](https://en.wikipedia.org/wiki/Euclidean_distance) distance between point(s) `a` and point `b`. _**Hint**: Array `a` can be an array of points. You may use the [`ndim` ğŸ”—](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.ndim.html) of a NumPy array to find its dimensionality._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(a, b):\n",
    "    '''Computes the Euclidean distance between point(s) A and another point B'''\n",
    "    \n",
    "    # your code here\n",
    "\n",
    "\n",
    "    assert isinstance(distance, (float, np.float64, np.ndarray)), 'Distance should be a float or a NumPy array'\n",
    "    assert True if not isinstance(distance, np.ndarray) else distance.shape[0] == a.shape[0], \\\n",
    "        'Should have the same number of distances as points in A'\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if your computation was right. Work out the Euclidean distance between $(0, 0)$ and $(2, 2)$ and compare that value to what your function returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean(np.zeros(2), np.array([2, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manhattan Distance\n",
    "\n",
    "Here's an example of an alternative distance measure. Imagine that you are actually trying to drive between two places in a city like New York.\n",
    "\n",
    "![manhattan](https://upload.wikimedia.org/wikipedia/commons/thumb/0/08/Manhattan_distance.svg/200px-Manhattan_distance.svg.png)\n",
    "\n",
    "How far do you have to go?\n",
    "\n",
    "**Try this!** Complete the `manhattan` function so that it computes the [Manhattan](https://en.wikipedia.org/wiki/Taxicab_geometry) distance between point(s) `a` and point `b`. `Hint` `a` can be an array of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan(a, b):\n",
    "    '''Computes the Manhattan distance between point(s) A and another point B'''\n",
    "\n",
    "    # your code here\n",
    "\n",
    "    \n",
    "    assert isinstance(distance, (float, np.float64, np.ndarray)), 'Distance should be a float or a NumPy array'\n",
    "    assert True if not isinstance(distance, np.ndarray) else distance.shape[0] == a.shape[0], \\\n",
    "        'Should have the same number of distances as points in A'\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check with your neighbors that your implementation works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan(np.zeros(2), np.array([2, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning Points to Clusters\n",
    "\n",
    "Okay, we can now get back to what we wanted to do: cluster. In this section we will implement a function that can take a point $x$ or a matrix $X$ and assign it or them to the cluster of the closest centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Complete the `assign` function so that it assigns each point to a cluster denoted by the index of the centroid. For example, if some point $x$ is closest to centroid $c_0$ then it should be assigned to cluster $0$. _**Hint**: One way to do this begins by creating an $n \\times k$ array of zeros to store distance values from each oberservation in `x` to each centroid in `centroids`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(x, centroids, distance_measure=euclidean):\n",
    "    '''\n",
    "    Computes the cluster assignments for X or each point\n",
    "    in X given some centroids and a distance measure\n",
    "    '''\n",
    "    \n",
    "    # your code here\n",
    "\n",
    "    \n",
    "    assert np.all(assignments >= 0) and np.all(assignments < len(centroids)), \\\n",
    "        'Assignments should be indices of centroids'\n",
    "    \n",
    "    return assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments = assign(X, centroids, distance_measure=euclidean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can compute the cluster assignments, let's plot and visualize them.\n",
    "\n",
    "**Try this!** Recreate the same plot you made before with the data points and the centroids, but this time color the data points based on their assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Centroids\n",
    "\n",
    "Now that we have some plausible assignments of our data points into clusters centered on the centroids, we can think about how we might improve the centroids. In the $k$-means algorithm, we compute new centroids by averaging the positions of all data points assigned to a cluster. This average point describes a \"central\" location within each cluster and thus makes sense as a representative for each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Complete the `compute_centroids` function so that it computes a new centroid for each cluster given the points assigned to each cluster by averaging those points. _**Hint**: Consider using `np.unique` to find all possible assignment values._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(X, assignments):\n",
    "    '''Computes new centroids given points X and cluster ASSIGNMENTS'''\n",
    "    \n",
    "    # your code here\n",
    "\n",
    "    \n",
    "    assert len(np.unique(assignments)) == len(centroids), \\\n",
    "        'You should have the same number of centroids as clusters'\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_centroids = centroids\n",
    "centroids = compute_centroids(X, assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we have all of the components of the $k$-means algorithm. Let's write a function that can plot all of thes things.\n",
    "\n",
    "**Try this!** Complete the `plot_kmeans` function so that it recreates the last plot that you made earlier, but this time, it can **optionally** scatter the `old_centroids` in a different color. `Hint` Check if an arg is `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kmeans(X, centroids, prev_centroids=None, assignments=None):\n",
    "    '''\n",
    "    Creates k-means plots\n",
    "    '''\n",
    "    \n",
    "    # your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kmeans(X, centroids, prev_centroids=old_centroids, assignments=assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Things Together\n",
    "\n",
    "With the last section, we finished all of the components that we need to construct the $k$-means algorithm. As you might have noticed, by repeating the assignment-cluster-mean cycle, we can get more and more central centroids. As we iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Complete the `fit` function, which should return the final centroids. Ensure that you make use of all of the supplied function arguments. If `initial=None` then you should generate some random centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, k, max_iters=1000, tol=1e-2, initial=None, random_state=None):\n",
    "    '''\n",
    "    Runs k-means cycle with data X and K clusters until either MAX_ITERS\n",
    "    number of iterations have been performed or until the absolute centroid delta\n",
    "    is less than TOL.\n",
    "    '''\n",
    "    \n",
    "    if initial is None:\n",
    "        centroids = sample_centroids(X, k, random_state=random_state)\n",
    "    else:\n",
    "        centroids = initial\n",
    "    \n",
    "    assert k == centroids.shape[0], \\\n",
    "        f'expected there to be {k} centroids but got {centroids.shape[0]}'\n",
    "    \n",
    "    # your code here\n",
    "\n",
    "            \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = fit(X, k)\n",
    "assignments = assign(X, centroids)\n",
    "\n",
    "plot_kmeans(X, centroids, assignments=assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** If you have a chance, before you move on, try using a different distance measure and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Initial Centroids Matter!\n",
    "\n",
    "The $k$-means algorithm is sensitive to the initial starting points. In this section, let's explore this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we generate a new data toy dataset with more clusters in order to make things more obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, k = 500, 5\n",
    "X, _ = load_toy(n, k, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Using the functions `sample_centroids`, `fit`, and `plot_kmeans`, experiment with different `random_state`s to see if you can observe different final centroids depending on the initial starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write-up!** Why might be causing this to happen? How might we better choose our initial centroids? Discuss your answer with your neighbors."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deciding How Many Clusters to Use\n",
    "\n",
    "As we have seen for $k$-means, $k$ is the number of clusters/centroids that the algorithm will try to find. Choosing $k$ is an important task as it determines the output of the algorithm. Since $k$ is a model parameter, we can try to use some good-ol' model selection to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the Sum of Squared Distances $SSD_j$ as the sum of all points in the $j$th cluster to its corresponsing cluster center $c_j$: \n",
    "$$SSD_j = \\sum_{i=1}^{n} z_{ij} \\;d(x_i,c_j),$$\n",
    "where $z_{ij}$ is 1 if $x_i$ belongs to cluster $j$ and 0 otherwise. \n",
    "\n",
    "Then, the objective function that $k$-means optimizes is the sum of the $SSD_j$ over all clusters. This means that, we want to find clusters of points that are close to one another. We can estimate how close the cluster points are to one another by measuring how far each point assigned to the cluster is from its center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we will generate a dataset with a random number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, k = 1000, np.random.randint(1, 11)\n",
    "X, y = load_toy(n, k, random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** In the following cell, `fit` multiple $k$-means centroids with $k \\in [1, 10]$, compute the sum of the $RSS_k$ for each cluster, and plot these values across $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write-up!** Describe how you would choose which $k$ to use. Then, choose the $k$ you would use going forward (if you had to)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. [Optional] Trying K-means On A Not-So-Simple Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Let's try using $k$-means on a different dataset. In the cell below, we prepare $X$ for you. Cluster the data and visualize the results. Use a `random_state` of 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, _ = make_moons(500, noise=0.1)\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write-up!** What do you notice about the clustering results from $k$-means? What are the implications of these results? Write your answers below and discuss them with your neighbors."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
